\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{hyperref}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Pn}{\mathbb{P}}
\newcommand{\indep}{\perp \!\!\! \perp}

\title{Formulario Probabilità e Statistica}
\author{Nicolas Alberti}
\date{Aggiornato al 20 Marzo 2021}

\begin{document}

\maketitle
\tableofcontents
\begin{abstract}
    Questo formulario è in continuo aggiornamento, almeno per l'anno accademico 2020/2021.\\
    Non sono presenti esempi o note aggiuntive riguardo le formule, perchè non so se sono accettate.\\
    Se ci sono errori, dubbi o aggiunte che vorreste fare, scrivete pure alla mail:\\
    \\
    \href{mailto:nicolas.alberti@studenti.unipd.it}{nicolas.alberti@studenti.unipd.it}
\end{abstract}
\chapter{Formulario}
\section{Operazioni Elementari}
\subsection{Complementare di un evento}
\[E^c = \{(\omega \in \Omega | \omega \ni)E\}\]
\subsection{Intersezione di due eventi}
\[E \cap F\ =\ \{\omega \in \Omega | \omega \in E\ e\ \omega \in F\}\]
\subsection{Eventi Incompatibili o Disgiunti}
\[E \cap F\ = \varnothing \]
\subsection{Unione di due eventi}
\[E \cup F\ =\ \{\omega \in \Omega | \omega\in E\ o\ \omega \in F\}\]
\subsection{Differenza Non Simmetrica}
\[E\backslash F\ = \{\omega \in \Omega\ |\ \omega \in E\ e\ \omega \not\in F\}\]
\subsection{Differenza Simmetrica}
\[E \triangle F\ =\ \{\omega \in \Omega\ |\ \omega \in (E\backslash F)\ o\ \omega \in (F \backslash E)\}\]
\section{Proprietà Fondamentali di Unione ed Intersezione}
\subsection{Leggi Commutative}
\[E \cup F\ =\ F \cup E\] ed anche \[E \cap F\ =\ F \cap E\]
\subsection{Leggi Associative}
\[(E \cup F) \cup G\ =\ E \cup (F \cup G)\] ed anche \[(E \cap F)\cap G\ =\ E \cap (F \cap G)\]
\subsection{Leggi Distributive}
\[E \cup (F \cap G)\ =\ (E \cup F)\cap (E \cup G)\] ed anche \[E \cap (F \cup G)\ =\ (E \cap F)\cup (E \cap G)\]
\subsection{Leggi di De Morgan}
\[(E \cup F)^C\ =\ E^C \cap F^C\] ed anche \[(E \cap F)^C\ = E^C \cup F^C\ \]
\section{Decomposzione di eventi}
\subsection{Decomposizione di un evento rispetto ad una partizione}
Siano F evento e \(\{E, E^C\}\) una partizione di \(\Omega\). Allora \[F = (F \cap E) \cup (F\cap E^C)\] Possiamo dire che \((F \cap E)\ e\ (F\cap E^C)\) sono eventi \textbf{disgiunti}.\\
In generale, se \(\{E_n\}_{n\geq 1}\) è una partizione di \(\Omega\), abbiamo \[F = \bigcup_{n\geq 1} (F \cap E_n)\]
\subsection{Decomposizione dell'unione di due eventi}
Siano \[E,F \subseteq \Omega\ eventi\] si possono avere due casi:\begin{enumerate}
    \item \[E \cup F = (E \backslash F)\cup (E \cap F)\cup (F\backslash E)\]
    \item \[E \cup F = E \cup (F \backslash E)\]
\end{enumerate}
\section{Sigma-Algebra degli eventi}
Una famiglia \textit{F} di sottoinsiemi di \(\Omega\) si chiama \(\sigma-algebra\) se: \begin{enumerate}
    \item \textit{F} \textbf{è non vuota}
    \item se \(E \in \textit{F}\), allora \(E^C \in F\)
    \item se \(E_n \in \textit{F}\) per \(n \geq 1\), allora \(\bigcup_{n \geq 1} E_n \in F\); identifica la chiusura per unione numerabile.
\end{enumerate}
\subsection{Conseguenze Elementari degli assiomi}
\begin{itemize}
    \item \(\Omega \in \textit{F}\ e\ \varnothing \in \textit{F}\)
    \item \((\bigcap_{n \geq 1}E_n)^C\ = \bigcup_{n \geq 1}E_n^C\ \in \textit{F}\)
    \item \(\bigcap_{n \geq 1}E_n = ((\bigcap_{n \geq 1}E_n)^C)^C\)
\end{itemize}
\section{Misura di Probabilità}
na misura di probabilità è una \textbf{mappa} \[P: \textit{F} \longrightarrow [01]\] in cui \[E \longmapsto P(E)\] dove \(P(E)\) è la probabilità dell'evento E, tale che soddisfi 2 proprietà: \begin{enumerate}
    \item \(P(\Omega) = 1\), è la \textbf{normalizzazione}
    \item se \(\{E_n\}_{n \geq 1}\) famiglia di eventi \textbf{mutualmente incompatibili} (ovvero\\ quando l'intersezione fra i,j di una famiglia è vuota quando \(i \neq j\)), allora \[P(\bigcup_{n \geq 1}E_n) = \sum_{n \geq 1} P(E_n)\]
\end{enumerate}
\subsection{Conseguenza degli assiomi sulla Misura di Probabilità}
\begin{enumerate}
    \item \(P(E^C) = 1-P(E)\)
    \item \(P(\varnothing) = 0\)
    \item Se \(E \subset F\), allora \(P(E) < P(F)\)
\end{enumerate}
\subsection{Formula di Inclusione/Esclusione}
Ci dà modo di calcolare la probabilità di una \textbf{unione di eventi}.
    \[P(E \cup F) = P(E) + P(F) - P(E \cap F)\]
\section{Scelta Misura Probabilità di P}
\subsection{Omega Discreto}
Consideriamo \((\Omega, \textit{F}, P)\), con \(\Omega\) discreto e \(\textit{F} = \Pn(\Omega)\). Come si assegna P su \textit{F}?\\
Se \(\Omega = \{\omega_1, \omega_2, ... \}\) è discreta, allora ogni evento \(E \subseteq \Omega\) (o, equivalentemente \(E \in \textit{F}\)) può essere scritto come \textbf{unione numerabile (o finita)} di elementi di \(\Omega\), cioè significa che: \[E = \{\omega_{i1}, \omega_{i2}, \omega_{i3}, ...\} = \{\omega_{i1}\} \cup \{\omega_{i2}\} \cup \{\omega_{i3}\} \cup ...\] (dove gli indici \textit{i} sono scelti precisamente) è una \textbf{unione disgiunta di singoletti}.\\
Se P fosse la probabilità a cui siamo interessati, allora avremmo che \[P(E) = P(\{\omega_{i1}) + P(\{\omega_{i2}) + P(\{\omega_{i3}) + ...\}\]
Se leggiamo dal \textbf{punto di vista opposto}: è sufficiente assegnare \[P(\{\omega_{i}\}) = p_i\] con i = 1, 2, 3, ... tali che: \begin{itemize}
    \item \(p_i \in [0, 1]\), quindi la \textbf{positività}
    \item \(\sum_{i \geq 1} p_i = 1\), quindi la \textbf{normalizzazione}
\end{itemize}
Quindi se \(\Omega\) è discreto, assegno le \textbf{proprietà ai singoletti} che poi andrò ad \textbf{unire} per ottenere la probabilità dell'evento P.
\subsection{Omega Finito}
Se \(\Omega = \{\omega_1, \omega_2, ..., \omega_N\}\) ha \textbf{cardinalità finita} \(\N\), possiamo scegliere la \textbf{misura di probabilità uniforme}, detta uniforme poichè assegna a \textbf{tutti gli esiti la stessa probabilità}, ovvero: \[P(\{\omega_1\}) = P(\{\omega_2\}) = ... = P(\{\omega_N\}) = \frac{1}{N}\] Questo mi dice che \textbf{tutti} gli esiti sono \textbf{equiprobabili}.\\
Per ogni evento \(E \in \textit{F}\), si ha \[P(E) = \sum_{\omega_i \in E} P(\{\omega_i\}) = |E| * \frac{1}{N} = \frac{|E|}{|\Omega|}\] (ovvero cardinalità di E fratto cardinalità di Omega). Questa ultima dicitura indica la famosa descrizione: \[\frac{Casi\ Favorevoli}{Casi\ Possibili}\]
\section{Combinatoria Elementare}
\subsection{Principio Fondamentale del Conteggio}
Sia E un insieme. Supponiamo che ogni elemento di E \textbf{si possa determinare univocamente mediante K scelte }successive, tali che: \begin{itemize}
    \item La prima scelta ha \(n_1\) esiti possibili
    \item La seconda scelta ha \(n_2\) esiti possibili
    \item La \textbf{K-esima} scelta ha \(n_k\) esiti possibili
\end{itemize}
allora la cardinalità di E sarà: \[|E| = n_1*n_2*...*n_k\]
\paragraph{Osservazione} Cosa vuol dire "determinare univocamente"? Vuol dire che \textbf{sequenze distinte di scelte individuano elementi diversi di E}.\\ 
E' importante, perchè \textbf{se non è soddisfatta} la cardinalità risulterà \textbf{errata!} e il P.F.C non varrà.
\subsection{Disposizioni con Ripetizione}
\paragraph{Scopo:} Contare le K-uple \textbf{ordinate} che posso creare scegliendo ogni entrata da n oggetti con la possibilità di ripetizione.\\
Ogni entrata può essere scelta tra \textbf{n alternative} (ci può essere \textbf{ripetizione}) e faccio \textbf{K scelte} totali, una per ogni entrata del vettore che voglio costruire. Quindi, per sapere il numero totale di disposizioni con ripetizione è: \[n^K\]
\subsection{Disposizioni Senza Ripetizione}
\paragraph{Scopo:} contare le K-uple \textbf{ordinate} (poichè sono disposizioni quindi l'ordine conta) che posso creare scegliendo ciascuna entrata da n oggetti senza possibilità di ripetizione.
\begin{itemize}
    \item La prima entrata può essere scelta tra \textbf{n alternative}
    \item La seconda tra \textbf{n-1 alternative}
    \item Fino alla \textbf{K-esima} che può essere scelta tra \textbf{n-k+1} alternative
\end{itemize}
Il numero delle disposizioni senza ripetizione sarà \(n*(n-1)*(n-2)*...*(n-k+1)\) con K fattori. Se \textbf{K = n} ottengo le \textbf{permutazioni} che contano tutti i modi possibili per ordinare gli n oggetti. In particolare si ottiene che il numero di possibili permutazioni è \[n!\]
\section{Probabilità Condizionata}
Siano \((\Omega, \textit{F}, P)\) uno spazio di probabilità e \(F \in \textit{F}\) un evento tale che \(P(F) > 0\).\\
Allora, per ogni altro evento \(E \in \textit{F}\) è ben definita la quantità \[P(E|F) = \frac{P(E \cap F)}{P(F)}\] Questa è la probabilità condizionata di \textbf{E dato F}.
\subsection{Formula di Moltiplicazione}
Se P(E) e P(F) sono entrambe strettamente positive, allora \[P(E \cap F) = P(E|F)*P(F) = P(F|E)*P(E)\]
\subsection{Teorema e Conseguenze}
Sia \(F \in \textit{F}\) un evento con \(P(F) > 0\), allora la mappa \[P(\cdot |F): \textit{F} \xrightarrow{} \mathbb{R}\] \[E \longmapsto P(E|F)\] è una misura di probabilità. Da ciò seguono due conseguenze importanti: \begin{enumerate}
    \item \(P(E^c|F) = 1- P(E|F)\)
    \item \(P(E \cup G|F = P(E|F) + P(G|F) - P(E \cap G|F)\), è la formula di \textbf{inclusione/esclusione}
\end{enumerate}
Inoltre: \[P(E|F^c) \neq 1 - P(E|F)\]
\subsection{Formula delle Probabilità Totali}
Siano (\(\Omega, \textit{F}, P\)) uno spazio di probabilità e \(F \in \textit{F}\) un evento tale che \(0 < P(F) < 1\), quindi un evento non impossibile e non certo, allora, per ogni \(E \in \textit{F}\), vale la formula delle probabilità totali: \[P(E) = P(E|F)\cdot P(F) + P(E|F^c)\cdot P(F^c)\] In generale, se ho \(\{F_k\}^n_{k = 1}\) che è una partizione di \(\Omega\) con \(P(F_k) > 0\) per ogni k, vale la formula delle probabilità totali: \[P(E) = \sum^n_{k=1} P(E|F_k)\cdot P(F_k)\]
\subsection{Formula di Bayes}
Questa formula serve per stimare le probabilità a posteriori, quindi a "rovesciare" il condizionamento. \[\frac{P(E|F_k)\cdot P(F_k)}{P(E)}\]
\section{Eventi Indipendenti}
\subsection{Indipendenza di due eventi}
Sia \((\Omega, \textit{F}, P)\) spazio di probabilità. Gli eventi \(E, F \in \textit{F}\) si dicono indipendenti se \[P(E \cap F) = P(E) \cdot P(F)\] e si scrive \[E \indep F\ o\ \{E, F\}\ indipendente\]
\paragraph{Attenzione} Indipendenza è \textbf{diverso da} incompatibilità. \begin{itemize}
    \item Indipendenza è una nozione \textbf{probabilistica}, dipende da E,F e dalla misura P.
    \item Incompatibilità è una nozione \textbf{insiemistica}.
\end{itemize}
In particolare, se E,F \(\in \textit{F}\) di probabilità strettamente positiva sono incompatibili, allora \textbf{non possono essere indipendenti}. Infatti, essendo incompatibili, significa che se uno si verifica l'altro \textbf{non può certamente verificarsi}, quindi c'è una \textbf{forte dipendenza} tra i due eventi.
\subsection{Conseguenze dell'indipendenza}
\paragraph{Parte 1}Siano E,F \(\in \textit{F}\) con \(P(E) > 0\ e\ P(F) > 0\). Allora le seguenti affermazioni:
    \begin{itemize}
        \item [(i)] \(E \indep F\)
        \item [(ii)] \(P(E|F) = P(E)\)
        \item [(iii)] \(P(F|E) = P(F)\)
    \end{itemize}
    sono \textbf{equivalenti}.
\paragraph{Parte 2}
Abbiamo \textbf{equivalenza} tra le seguenti istanze:
\begin{itemize}
    \item [(i)] \(E \indep F\)
    \item [(ii)] \(E^c \indep F\)
    \item [(iii)] \(E^c \indep F^c\)
    \item [(iv)] \(E \indep F^c\)
\end{itemize}
\subsection{Indipendenza di Tre Eventi}
Gli eventi \(E_1, E_2, E_3\) sono indipendenti se le seguenti due condizioni sono \textbf{entrambe} soddisfatte: \begin{enumerate}
    \item \(E_1 \indep E_2, E_1 \indep E_3, E_2 \indep E_3\)
    \item \(P(E_1 \cap E_2 \cap E_3) = P(E_1)\cdot P(E_2) \cdot P(E_3)\)
\end{enumerate}
\subsubsection{In generale}
La famiglia di \(\{E_1,...,E_n\}\) di eventi è indipendente se \textbf{per ogni r} (\(2 \leq r \leq n\)) e per \textbf{ogni possibile scelta di r eventi distinti} degli n eventi della famiglia, la probabilità dell'intersezione degli r eventi scelti è \textbf{pari al prodotto delle loro probabilità}.\\
Devo quindi dimostrare la \textbf{proprietà di fattorizzazione} per ogni coppia, terna, quaterna, ecc. di eventi. Devo considerare \textbf{tutte le possibili sottofamiglie}.
\end{document}

