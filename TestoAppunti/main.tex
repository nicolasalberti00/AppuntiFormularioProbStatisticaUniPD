\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{hyperref}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Pn}{\mathbb{P}}
\newcommand{\indep}{\perp \!\!\! \perp}

\title{Appunti Probabilità e Statistica}
\author{Nicolas Alberti}
\date{Anno Accademico 2020/21}


\begin{document}
\maketitle
\begin{abstract}
    Questi sono appunti redatti seguendo le video-lezioni. \\
    Dovrebbero contenere tutto, quindi dovrebbero essere pronti per lo studio finale.
\end{abstract}
\tableofcontents

\chapter{Lezione 1}

\section{Spazio di Probabilità}

\subsection{Definizione:}

Uno spazio di probabilità è una terna (\(\Omega\), F, P) dove:

\begin{itemize}
    \item \(\Omega\): è un Insieme
    \item F: è una famiglia di sottoinsiemi di \(\Omega\) con una particolare struttura detta \(\sigma\)-algebra
    \item P: da F \(\xrightarrow{}[0, 1]\), è una mappa con misura positiva e normalizzata
\end{itemize}

Descriviamo ora nel particolare il primo simbolo.
\(\Omega\) è lo \textbf{spazio campionario}, ovvero è l'insieme di \textbf{tutti gli esiti} dell'esperimento aleatorio che stiamo considerando.
    \paragraph{Oss.1 :} bisogna scegliere un insieme \(\Omega\) sufficientemente ricco capace di contenere tutti i risultati dell'esperimento considerato. Quindi la scelta dello spazio campionario \textbf{non è} univoca. \\
    \paragraph{Oss.2 :}\begin{itemize}
        \item Se \((\Omega)\) \(\leq\) \(\mathrm{N}\), si dice che lo spazio campionario è \textbf{discreto}.
        \item Se \((\Omega\)) \(\geq\) \(\mathrm{N}\), si dice che lo spazio campionario è \textbf{continuo}.
    \end{itemize}
\subsection{Esempi:}
\begin{enumerate}
    \item Lancio 1 moneta, osservo la faccia uscita: 
    \[\Omega = \{T,C\}\] dove T e C sono Testa e Croce
    \item Lancio 1 moneta per 3 volte, vado a contare il numero di Teste uscite: 
    \[\Omega = \{0, 1, 2, 3\}\]
    \item Lancio 1 moneta per 3 volte, ora osservo la sequenza delle facce ottenute:
    \[\Omega = \{TTT, TTC, TCT, CTT, TCC, CTC, CCT, CCC\}\]
    \item Lancio di 1 moneta fino a che non ottengo Testa e conto il numero di lanci effettuati: 
    \[\Omega = \mathrm{N+} = \{1, 2, 3, ...\}\]
    \item Considero il tempo di vita di un Hard Disk:
    \[\Omega = \mathrm{R+} = [0, +\infty]\]
\end{enumerate}
\subsection{Esiti ed Eventi:}
Esiti ed Eventi sono degli \textbf{elementi speciali} dei sottoinsiemi.
\begin{itemize}
    \item se \(\omega\) \(\in\) \(\Omega\) (ovvero \(\omega\) è \textbf{elemento} di \(\Omega\)), allora è detto \textbf{esito} (o \textit{evento elementare}
    \item se E \(\subseteq\) \(\Omega\) (ovvero E è \textbf{sottoinsieme} di \(\Omega\), allora è detto \textbf{evento} 
\end{itemize}

Se l'esecuzione dell'esperimento dà come risultato \(\omega\) \(\in\) \(\Omega\), diremo che si è verificato \(\omega\). \\

Per ogni E tale che \(\omega\) \(\in\) E, allora diremo che si è verificato E.

\subsection{Esempio:}
Riprendiamo l'esempio 3 descritto prima. Lo spazio campionario che rappresenta i possibili risultati di 3 lanci di una moneta è : \[\Omega = \{TTT, TTC, TCT, CTT, TCC, CTC, CCT, CCC\}\]

Gli eventi possibili saranno quindi:
\begin{itemize}
    \item \(E_k\) = "ottengo K croci (in 3 lanci) ", con K = 0, 1, 2, 3
    Per ogni K devo esaminarne gli eventi \textbf{favorevoli}:
    \begin{enumerate}
        \item \(E_0\) = {TTT}
        \item \(E_1\) = {TCT, CTT, TTC}
        \item \(E_2\) = {TCC, CTC, CCT}
        \item \(E_3\) = {CCC}
    \end{enumerate}
    Supponiamo di avere ottenuto come esito, dopo i 3 lanci, CTC: si verifica quindi \(E_2\) ma non si verificano \(E_0\), \(E_1\) ed \(E_3\).
\end{itemize}

\subsection{Eventi banali ed onnipresenti:}
Sono 2 e sono quelli di seguito:
\begin{itemize}
    \item \(\Omega\): evento \textbf{certo}
    \item \(\emptyset\): evento \textbf{impossibile}
\end{itemize}

\section{Operazioni elementari sugli eventi}

Le operazioni insiemistiche permettono di combinare più spazi campionari, al fine di ottenere eventi più interessanti.

Siano E, F \(\in \Omega\) degli eventi. Abbiamo:
\begin{itemize}
    \item il \textbf{complementare} di E (evento):
    \[E^c = \{(\omega \in \Omega | \omega \ni)E\}\]
    E' l'insieme degli esiti che \textbf{non stanno} in E.
    Probabilisticamente, \(E^c\) si verifica quando non si verifica E.
    \item \textbf{Intersezione} di E con F: \[E \cap F\ =\ \{\omega \in \Omega | \omega \in E\ e\ \omega \in F\}\] Probabilisticamente, \(E \cap F\) si verifica solo se si verificano \textbf{sia E che F}. Se E ed F sono tali che: \[E \cap F\ = \varnothing \] si dice che E ed F sono \textbf{incompatibili} o \textbf{disgiunti}, cioè E ed F non si possono verificare allo stesso momento.
    \item \textbf{Unione} di E con F: \[E \cup F\ =\ \{\omega \in \Omega | \omega\in E\ o\ \omega \in F\}\] Probabilisticamente, \(E \cup F\) si verifica se \textbf{almeno uno} tra E ed F si verifica. L'unione corrisponde ad una \textbf{"o" inclusiva}.
    \item \textbf{Differenza} tra insiemi. Ne esistono di 2 tipi:
    \begin{itemize}
        \item \(E\backslash F\) \textit{(si legge E meno F)} \textbf{non simmetrica}: significa che (\(F\backslash E \neq E\backslash F\)). \[E\backslash F\ = \{\omega \in \Omega\ |\ \omega \in E\ e\ \omega \not\in F\}\]
        Probabilisticamente, si dice che \(E\backslash F\) si verifica quando si verifica E \textbf{ma non si verifica} F.
        \item \(E \triangle F\): è la differenza \textbf{simmetrica}: \[E \triangle F\ =\ \{\omega \in \Omega\ |\ \omega \in (E\backslash F)\ o\ \omega \in (F \backslash E)\}\] Probabilisticamente, si dice che \(E \triangle F \) si verifica se \textbf{esattamente uno} tra E ed F si verifica. Corrisponde ad una \textbf{"o" esclusiva}.
    \end{itemize}
\end{itemize}
\paragraph{Esempio} 
Vediamo l'esempio dei 3 lanci della moneta visto in precedenza. Abbiamo definito: \(E_K\ =\ "Ottengo\ K\ croci",\ K\ =\ 0,1,2,3. \). Abbiamo un evento \[E\ =\ "esce\ almeno\ una\ testa"\ =\ E{^C_3}\] il quale è il \textbf{complementare} di \[F\ =\ "escono\ almeno\ 2\ croci"\ =\ E_2 \cup E_3\] Abbiamo anche l'evento \[G\ =\ "escono\ almeno\ due\ teste"\ =\ (E_2 \cup E_3)^C\]
\section{Proprietà fondamentali di unione ed intersezione}
Esistono varie leggi
\begin{itemize}
    \item Leggi \textbf{commutative} \[E \cup F\ =\ F \cup E\] ed anche \[E \cap F\ =\ F \cap E\]
    \item Leggi \textbf{associative} \[(E \cup F) \cup G\ =\ E \cup (F \cup G)\] ed anche \[(E \cap F)\cap G\ =\ E \cap (F \cap G)\]
    \item Leggi \textbf{distributive} \[E \cup (F \cap G)\ =\ (E \cup F)\cap (E \cup G)\] ed anche \[E \cap (F \cup G)\ =\ (E \cap F)\cup (E \cap G)\]
    \item Leggi di \textbf{De Morgan} \[(E \cup F)^C\ =\ E^C \cap F^C\] ed anche \[(E \cap F)^C\ = E^C \cup F^C\ \]
\end{itemize}

\section{Decomposizioni in unioni disgiunte}
E' un metodo conveniente quando si tratteranno le probabilità. Dobbiamo innanzitutto \textbf{ripartire lo spazio campionario}.
\begin{itemize}
    \item \textbf{Partizione di \(\Omega\)}: E' una famiglia di eventi \(\{E_N\}_{N\geq1}\). Gli eventi sono \textbf{a 2 a 2 disgiunti} e la proprietà è anche detta \textit{mutua disgiunzione} o \textit{mutua incompatibilità} e la loro unione porta ad ottenere lo spazio campionario \(\Omega\) di partenza. Simbolicamente diciamo:
    \begin{itemize}
        \item \(E_i \cap E_j = \varnothing\) per \(i \neq j\)
        \item \(\bigcup_{N\geq 1} E_N = \Omega\)
    \end{itemize}
    Significa che fra gli eventi \textbf{non deve esserci intersezione} e quando verranno riuniti essi copriranno \textbf{tutto lo spazio campionario}.

\paragraph{Esempi} \begin{enumerate}
    \item Se \(E \subseteq \Omega\) è un evento, allora la famiglia \(\{E, E^C\}\) è una partizione di \(\Omega\).
    \item Prendiamo di nuovo l'esempio dei 3 lanci di una moneta. La famiglia che costruiamo con gli eventi \(E_K = \{E_0,E_1,E_2,E_3\}\) (che, ricordiamo, sono l'ottenimento di K croci), questa famiglia \textbf{è una partizione di} \(\Omega\).
\end{enumerate}
\item \textbf{Decomposizione di un evento rispetto ad una partizione}: Siano F evento e \(\{E, E^C\}\) una partizione di \(\Omega\). Allora \[F = (F \cap E) \cup (F\cap E^C)\] Possiamo dire che \((F \cap E)\ e\ (F\cap E^C)\) sono eventi \textbf{disgiunti}.
\clearpage
\begin{figure}[hb]
    \centering
    \includegraphics[width = 6cm]{EventiDisgiuntiinOmega.png}
    \caption{Questa è la rappresentazione grafica di ciò che abbiamo appena descritto}
    \label{fig:EventiDisgiunti}
\end{figure}
In generale, se \(\{E_n\}_{n\geq 1}\) è una partizione di \(\Omega\), abbiamo \[F = \bigcup_{n\geq 1} (F \cap E_n)\]

\item \textbf{Decomposizione dell'unione di due eventi}: serve a riscrivere un unione in modo che gli eventi che vado ad unire per rappresentarla siano \textbf{a due a due disgiunti}. Non esiste una regola ed esistono varie decomposizioni. Siano \[E,F \subseteq \Omega\ eventi\] si può avere il primo caso (\ref{fig:DecomposizioneC1})\[E \cup F = (E \backslash F)\cup (E \cap F)\cup (F\backslash E)\]
Altrimenti possiamo avere anche il secondo caso (\ref{fig:DecomposizioneC2}): \[E \cup F = E \cup (F \backslash E)\]
\begin{figure}
    \centering
    \includegraphics[width = 6cm]{DecomposizioneC.png}
    \caption{Ecco la raffigurazione del primo esempio di decomposizione: la parte blu indica \((E\backslash F)\), la parte rossa indica \((E \cap F)\) mentre la parte verde indica \((F\backslash E)\). Interessante notare come questi tre eventi siano anche \textbf{disgiunti} tra loro.}
    \label{fig:DecomposizioneC1}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width = 6cm]{DecomposizioneC2.png}
    \caption{Ecco la raffigurazione del secondo esempio. In verde abbiamo E, mentre in blu abbiamo \((F \backslash E)\). Questi due eventi restano \textbf{disgiunti}.}
    \label{fig:DecomposizioneC2}
\end{figure}
\end{itemize}

\chapter{Lezione 2}
\section{La sigma-algebra degli Eventi}
Una famiglia \textit{F} (ricorda F corsivo) di sottoinsiemi di \(\Omega\) si chiama \(\sigma-algebra\) se: \begin{enumerate}
    \item \textit{F} \textbf{è non vuota}
    \item se \(E \in \textit{F}\), allora \(E^C \in F\)
    \item se \(E_n \in \textit{F}\) per \(n \geq 1\), allora \(\bigcup_{n \geq 1} E_n \in F\); identifica la chiusura per unione numerabile.
\end{enumerate}
\subsection{Esempi}
Vediamo alcuni esempi: \begin{enumerate}
    \item \(\sigma-algebra\) \textbf{banale}: qualunque sia \(\Omega\), la famiglia \(\{\Omega, \varnothing\}\) è una \(\sigma-algebra\). E' quindi \(\sigma-algebra\) lo spazio campionario \(\Omega\) e lo spazio vuoto \(\varnothing\).
    \item \(\sigma-algebra\) \textbf{generata da un evento}: dato \(\Omega\) e un evento \(E \subseteq \Omega\), la famiglia \(\{\varnothing, E, E^C, \Omega\}\) è una \(\sigma-algebra\).
    \item \(\sigma-algebra\) \textbf{massima}: qualunque sia \(\Omega\), la famiglia di tutti i suoi sottoinsiemi (detta \textbf{insieme delle parti}) \[\textit{F} = \Pn(\Omega)\] è una \(\sigma-algebra\). E' la più grande \(\sigma-algebra\) costruibile. \paragraph{Nota:} se \(\Omega\) è discreto, prenderemo sempre \(\textit{F} = \Pn(\Omega)\) 
\end{enumerate}
\subsection{Conseguenze elementari degli assiomi}
Vediamo alcune conseguenze degli assiomi appena descritti: \begin{enumerate}
    \item \(\Omega \in \textit{F}\ e\ \varnothing \in \textit{F}\)\\ Come lo vediamo? Esiste un \(E \in \textit{F}\) per la condizione 1. vista prima, ciò implica \(\longrightarrow E^C \in \textit{F}\) per la condizione 2. di prima.\\ 
    Ora vediamo \(\Omega = \{E \cup E^C\} \in \textit{F}\), poichè la loro unione determina un numero \textbf{finito} di elementi, dato dalla condizione 3. vista prima. In conclusione diciamo anche che \(\Omega \in \textit{F}\) e anche che \(\varnothing = \Omega^C \in \textit{F}\) per la condizione 2.
    \item Chiusura per \textbf{intersezione numerabile}: dice che se ho \(E_n \in \textit{F}\) per ogni \(n \geq 1\), allora anche \(\bigcap_{n \geq 1}E_n \in \textit{F}\) sarà nella \(\sigma-algebra\).\\
    Come la vediamo? Devo riscrivere questa intersezione in un modo che mi permetta di utilizzare tutte le informazioni che ho, che sono solo su \textbf{unione e complementazione} di eventi sulla \(\sigma-algebra\). Sfrutto le \textbf{Leggi di De Morgan}: \begin{itemize}
        \item \((\bigcap_{n \geq 1}E_n)^C\ = \bigcup_{n \geq 1}E_n^C\ \in \textit{F}\).\\ Gli \(E_n^C \in \textit{F}\) per la \textbf{condizione 2.}. La loro \textbf{unione numerabile} \(\in \textit{F}\) per la \textbf{condizione 3.}
        \item Chiudiamo dicendo: \(\bigcap_{n \geq 1}E_n = ((\bigcap_{n \geq 1}E_n)^C)^C\) dove la \textbf{prima intersezione} complementare \(\in \textit{F}\)per il primo punto di questo elenco, ed il \textbf{suo complementare} \(\in \textit{F}\) per la \textbf{condzione 2.} della definizione precedente. Quindi l'oggetto è un elemento della \(\sigma-algebra\).
    \end{itemize}
\end{enumerate}
\section{Misura di Probabilità P}
\paragraph{Definizione} Una misura di probabilità è una \textbf{mappa} \[P: \textit{F} \longrightarrow [01]\] in cui \[E \longmapsto P(E)\] dove \(P(E)\) è la probabilità dell'evento E, tale che soddisfi 2 proprietà: \begin{enumerate}
    \item \(P(\Omega) = 1\), è la \textbf{normalizzazione}
    \item se \(\{E_n\}_{n \geq 1}\) famiglia di eventi \textbf{mutualmente incompatibili} (ovvero\\ quando l'intersezione fra i,j di una famiglia è vuota quando \(i \neq j\)), allora \[P(\bigcup_{n \geq 1}E_n) = \sum_{n \geq 1} P(E_n)\]
\end{enumerate}
La misura, come detto nella prima lezione, deve essere \textbf{positiva e normalizzata}. \begin{itemize}
    \item La \textbf{positività} è implicita nel fatto che la probabilità prende \textbf{valori nell'intervallo [0,1]}, quindi certamente positivi
    \item La condizione di \textbf{normalizzazione} invece è la condizione sulla probabilità dell'\textbf{intero spazio campionario}, come indicato prima.
\end{itemize}
\subsection{Conseguenze elementari degli assiomi}
\begin{enumerate}
    \item \(P(E^C) = 1-P(E)\): come lo vediamo?\\
    Si ha che \(1 = P(\Omega) = P(E \cup E^C) =^{per\ condizione\ 2\ definizione} P(E) + P(E^c)\), dove \(E\ ed\ E^c\) sono \textbf{disgiunti}.\\
    Da cui quindi otteniamo \[P(E^c) = 1 - P(E)\]
    \item \(P(\varnothing) = 0\): come lo vediamo?\\
    Si ha \(\varnothing = \Omega^c\), quindi posso usare la prima proprietà appena definita per dire: \[P(\varnothing) = P(\Omega^c) =^{(per\ proprieta'\ 1)} 1 - P(\Omega)_{(= 1\ per\ condizione\ 1)} = 0\]
    \item Se \(E \subset F\), allora \(P(E) < P(F)\): come lo vediamo?\\
    Se \(E \subset F\), allora \(F = E \cup (F\backslash E)\), dove i due eventi che sto unendo sono \textbf{disgiunti}. Quindi \[P(F) =^{(per\ proprieta'\ 2\ Misura\ delle\ Probabilita')} P(E) + P(F\backslash E)\] %disegno
    \(P(F\backslash E) \in [0,1]\), che è \textbf{positivo}, da cui segue \[P(F) > P(E)\] 
    \item Formula di \textbf{inclusione/esclusione}: ci dà modo di calcolare la probabilità di una \textbf{unione di eventi}.
    \[P(E \cup F) = P(E) + P(F) - P(E \cap F)\] Come la vedo?\\
    Si ha \(E \cup F = E \cup (F\backslash E)\) (unione disgiunta) e quindi \(P(E \cup F) = P(E) + P(F\backslash E)\).\\
    Inoltre \(F = (F\backslash E) \cup (F \cap E)\) (unione disgiunta) e quindi \(P( F) = P(F\backslash E) + P(F \cap E)\).\\
    Ricavo che: \(P(F\backslash E) = P(F) - P(F \cap E)\). Lo sostituisco nella probabilità dell'unione scritta inizialmente ed ottengo: \[P(E \cup F) = P(E) + P(F) - P(F \cap E) \]
\end{enumerate}
\subsection{Osservazione:} Dato (\(\Omega, \textit{F}\)) per completare la terna, la scelta della misura di probabilità P \textbf{non è univocamente determinata}, poichè gli assiomi \textbf{non determinano} un' unica misura di probabilità.
\paragraph{Esempio:} Consideriamo il lancio di una moneta: \[\Omega = \{T,C\}\ e\ \textit{F} = \Pn(\Omega)\]
Abbiamo \textbf{infinite} misure di probabilità compatibili con gli assiomi: \[(T) = p \in [0,1]\ ,\ P(C) = 1 - p\]
\chapter{Lezione 3}
\section{Scelta della Misura di Probabilità P}
\subsection{Scelta di P nel caso di Omega Discreto}
Consideriamo \((\Omega, \textit{F}, P)\), con \(\Omega\) discreto e \(\textit{F} = \Pn(\Omega)\). Come si assegna P su \textit{F}?\\
Se \(\Omega = \{\omega_1, \omega_2, ... \}\) è discreta, allora ogni evento \(E \subseteq \Omega\) (o, equivalentemente \(E \in \textit{F}\)) può essere scritto come \textbf{unione numerabile (o finita)} di elementi di \(\Omega\), cioè significa che: \[E = \{\omega_{i1}, \omega_{i2}, \omega_{i3}, ...\} = \{\omega_{i1}\} \cup \{\omega_{i2}\} \cup \{\omega_{i3}\} \cup ...\] (dove gli indici \textit{i} sono scelti precisamente) è una \textbf{unione disgiunta di singoletti}.\\
Se P fosse la probabilità a cui siamo interessati, allora avremmo che \[P(E) = P(\{\omega_{i1}) + P(\{\omega_{i2}) + P(\{\omega_{i3}) + ...\}\]
Se leggiamo dal \textbf{punto di vista opposto}: è sufficiente assegnare \[P(\{\omega_{i}\}) = p_i\] con i = 1, 2, 3, ... tali che: \begin{itemize}
    \item \(p_i \in [0, 1]\), quindi la \textbf{positività}
    \item \(\sum_{i \geq 1} p_i = 1\), quindi la \textbf{normalizzazione}
\end{itemize}
Quindi se \(\Omega\) è discreto, assegno le \textbf{proprietà ai singoletti} che poi andrò ad \textbf{unire} per ottenere la probabilità dell'evento P.
\subsubsection{Esempio}
Consideriamo \(\Omega = \N\) e definiamo \(P(\{i\}) = \frac{1}{2^i} \) per ogni \(i \in \N\). Questa è \textbf{una misura di probabilità}. Infatti verifico: \begin{itemize}
    \item \(0 < \frac{1}{2^i} \leq 1\)
    \item \(\sum_{\frac{1}{2^i}} = -1 + \sum_{i \geq 0}\frac{1}{2^i} = -1 + \frac{1}{1 - \frac{1}{2}} = 1 \)\\ 
    (Piccolo ripasso \textbf{serie geometrica}: \(\sum_{K \geq 0}x^K\), dove x è detta \textbf{ragione}, abbiamo che: \begin{itemize}
        \item \textbf{converge} se \(0 < x < 1\)
        \item \textbf{diverge} altrimenti
    \end{itemize}
    Se converge \(\sum_{K \geq 0}x^K = \frac{1}{1-x}\))
\end{itemize}
Possiamo utilizzare questa misura di probabilità per calcolare la probabilità dell'insieme dei multipli di 3 . Si ha infatti: \[P(\{3, 6, 9, 12, ...\}) = P(\{3*i\ con\ i \in \N\}) = \] \[= \sum_{i \geq 1} \frac{1}{2^{3i}} = \sum_{i \geq 1} \frac{1}{8^i} = -1 + \sum_{i \geq 0} \frac{1}{8^i} = -1 + \frac{1}{1 - \frac{1}{8}} = \frac{1}{7}\]
\subsection{Scelta di P nel caso di Omega Finito}
Se \(\Omega = \{\omega_1, \omega_2, ..., \omega_N\}\) ha \textbf{cardinalità finita} \(\N\), possiamo scegliere la \textbf{misura di probabilità uniforme}, detta uniforme poichè assegna a \textbf{tutti gli esiti la stessa probabilità}, ovvero: \[P(\{\omega_1\}) = P(\{\omega_2\}) = ... = P(\{\omega_N\}) = \frac{1}{N}\] Questo mi dice che \textbf{tutti} gli esiti sono \textbf{equiprobabili}.\\
Lo spazio di probabilità che ottengo scegliendo \(\Omega finito, \sigma-algebra\ delle\ parti\) e spazio di probabilità uniforme è detto \textbf{spazio di probabilità uniforme}. Come faccio a calcolare la probabilità di un evento E?\\
Per ogni evento \(E \in \textit{F}\), si ha \[P(E) = \sum_{\omega_i \in E} P(\{\omega_i\}) = |E| * \frac{1}{N} = \frac{|E|}{|\Omega|}\] (ovvero cardinalità di E fratto cardinalità di Omega). Questa ultima dicitura indica la famosa descrizione: \[\frac{Casi\ Favorevoli}{Casi\ Possibili}\]
\subsubsection{Esempio}
Consideriamo un mazzo di carte da poker (52 carte) e peschiamo una carta. \begin{itemize}
    \item La probabilità di estrarre una regina è \(\frac{4}{52}\)
    \item La probabilità di estrarre una carta di cuori è \(\frac{13}{52}\)
    \item La probabilità di estrarre la regina di cuori è \(\frac{1}{52}\) 
\end{itemize}
\subsubsection{Esercizio}
Lanciamo due dadi equilibrati. \begin{enumerate}
    \item \(E_1 =\) "Qual è la probabilità che la somma dei punteggi sia 8?" 
    \item \(E_2 =\) "Qual è la probabilità che escano due punteggi uguali?"
    \item \(E_3 =\) "Qual è la probabilità di ottenere 8 con due dadi con lo stesso punteggio?"
\end{enumerate} 
\paragraph{Soluzione} Lo spazio campionario è \[\Omega = \{(i, j):\ i,j \in \{1, 2, ..., 6\}\}\] cioè l'insieme delle coppie dei punteggi ottenuti sui due dadi.
Si ha \(|\Omega| = 6*6 = 36\)
\begin{enumerate}
    \item Consideriamo l' evento \[E_1 = \{(2,6), (3,5), (4,4), (5,3), (6,2)\}\] si ha \[P(E) = \frac{|E|}{|\Omega|} = \frac{5}{36}\]
    \item Consideriamo l'evento \[E_2 = \{(1,1), (2,2), (3,3), (4,4), (5,5), (6,6)\}\] si ha \[P(E) = \frac{|E|}{|\Omega|} = \frac{6}{36}\]
    \item Consideriamo l'evento \[E_3 = E_1 \cap E_2 = \{(4,4)\}\] si ha \[P(E) = \frac{|E|}{|\Omega|} = \frac{1}{36}\]
\end{enumerate}
\paragraph{Punto chiave:} Il punto chiave è il \textbf{contare gli elementi di un insieme}.
\section{Combinatoria Elementare}
\subsection{Principio Fondamentale del Conteggio (P.F.C)}
Sia E un insieme. Supponiamo che ogni elemento di E \textbf{si possa determinare univocamente mediante K scelte }successive, tali che: \begin{itemize}
    \item La prima scelta ha \(n_1\) esiti possibili
    \item La seconda scelta ha \(n_2\) esiti possibili
    \item La \textbf{K-esima} scelta ha \(n_k\) esiti possibili
\end{itemize}
allora la cardinalità di E sarà: \[|E| = n_1*n_2*...*n_k\]
\subsubsection{Esempio}
Contiamo le carte in un mazzo da poker, usando il P.F.C. Ogni carta del mazzo è individuata da un seme (C, Q, F, P) e da un valore (A, 2, 3, ..., J, Q, K). Per individuare una carta, cosa faccio? \begin{enumerate}
    \item Scelgo il \textbf{seme}: \textbf{4 esiti possibili}
    \item Scelgo il \textbf{valore}: \textbf{13 esiti possibili}
\end{enumerate}
Quindi otteniamo che il numero di carte nel mazzo è: \[4*13 = 52\]
\paragraph{Osservazione} Cosa vuol dire "determinare univocamente"? Vuol dire che \textbf{sequenze distinte di scelte individuano elementi diversi di E}.\\ 
E' importante, perchè \textbf{se non è soddisfatta} la cardinalità risulterà \textbf{errata!} e il P.F.C non varrà.
\subsubsection{Esempio 2}
Si deve formare un comitato di 2 persone da scegliersi dal gruppo \(\{U, D_1, D_2\}\) (dove U = Uomo e D = Donna). I possibili comitati sono 3: \begin{enumerate}
    \item \(\{U, D_1\}\)
    \item \(\{U, D_2\}\)
    \item \(\{D_1, D_2\}\)
\end{enumerate}
Ora ragioniamo come segue. Scegliere un comitato equivale a fare le seguenti scelte: \begin{enumerate}
    \item Scelgo una delle due donne: ho 2 esiti possibili
    \item Scelgo la persona che manca tra i rimanenti: ho 2 esiti possibili.
\end{enumerate}
Applico il P.F.C ed ottengo che il numero dei comitati è \(2*2 = 4\), ma \textbf{in realtà} i comitati \textbf{sono solo 3}. Dove sta l'errore?\\
Sta nel fatto che ci sono \textbf{2 distinte sequenze di scelte} che mi danno come risultato il comitato \(\{D_1, D_2\}\) e sono: \begin{itemize}
    \item \(\{D_1, D_2\}\)
    \item \(\{D_2, D_1\}\)
\end{itemize}
\chapter{Lezione 4}
\section{Disposizioni con Ripetizione}
\paragraph{Scopo:} Contare le K-uple \textbf{ordinate} che posso creare scegliendo ogni entrata da n oggetti con la possibilità di ripetizione.\\
Ogni entrata può essere scelta tra \textbf{n alternative} (ci può essere \textbf{ripetizione}) e faccio \textbf{K scelte} totali, una per ogni entrata del vettore che voglio costruire. Quindi, per sapere il numero totale di disposizioni con ripetizione è \[n^K\]
\subsubsection{Esempi}
\begin{enumerate}
    \item Una cassaforte con codice a 7 cifre ha \(10^7\) codici possibili.
    \item Metodo per generare sottoinsiemi di un dato insieme: se E è un insieme \textbf{finito}, con cardinalità \textbf{K}, allora i possibili sottoinsiemi di E sono \[2^K\]
    Infatti, possiamo specificare un sottoinsieme F di E nel seguente modo: ad ogni elemento di F possiamo assegnare:\begin{itemize}
        \item Il valore \textbf{1} se \textbf{sta in F}
        \item Il valore \textbf{0} se \textbf{non sta in F}
    \end{itemize}
    Ogni stringa di bit \(\{0, 1\}\) di lunghezza K \textbf{codifica un sottoinsieme di E}.\\
    \paragraph{Esempio:} E = \(\{+, -, *, :\}\), \(|E| = 4\) con stringa di bit: 0101 che genera il sottoinsieme: \[\{-, :\}\] poichè i valori corrispondono esattamente agli elementi in E \(("+" = 0\ quindi\ no, "-" = 1\ quindi\ si, eccetera.)\)
    Per contare i sottoinsiemi devo quindi \textbf{contare le stringhe}. Quante stringhe di lunghezza K posso formare scegliendo le entrate in \(\{0, 1\}\)?\\ Sono \(2^K\).
\end{enumerate}
\section{Disposizioni senza Ripetizione}
\paragraph{Scopo:} contare le K-uple \textbf{ordinate} (poichè sono disposizioni quindi l'ordine conta) che posso creare scegliendo ciascuna entrata da n oggetti senza possibilità di ripetizione.
\begin{itemize}
    \item La prima entrata può essere scelta tra \textbf{n alternative}
    \item La seconda tra \textbf{n-1 alternative}
    \item Fino alla \textbf{K-esima} che può essere scelta tra \textbf{n-k+1} alternative
\end{itemize}
Il numero delle disposizioni senza ripetizione sarà \(n*(n-1)*(n-2)*...*(n-k+1)\) con K fattori. Se \textbf{K = n} ottengo le \textbf{permutazioni} che contano tutti i modi possibili per ordinare gli n oggetti. In particolare si ottiene che il numero di possibili permutazioni è \[n!\]
\subsubsection{Esempio}
Cinque amici fanno una gara di nuoto. Le possibili classifiche sono \[5! = 120\] ovvero tutti i possibili ordinamenti dei nomi.\\
I possibili podii sono \[5*4*3 = 60\] rispettivamente 5 per l'oro, 4 per l'argento e 3 per il bronzo.
\section{Esercizi}
\begin{enumerate}
    \item Un'urna contiene palline numerate da 1 a 50. Si estraggono contemporaneamente 2 palline. Calcolare la probabilità di ottenere: \begin{itemize}
        \item [(a)] Due numeri dispari
        \item [(b)] Un numero divisibile per 5 e uno non divisibile per 5
        \item [(c)] Due numeri la cui somma è 50.
    \end{itemize}
    \paragraph{Soluzione} Considero \(\Omega\) l'insieme di combinazioni di 2 palline scelte tra le 50 nell'urna, senza tenere conto dell'ordine. Si ha \(|\Omega| = \binom{50}{2} = 1225\)\\
    Il coefficiente binomiale \(\binom{n}{k}\) è \[\frac{k!}{(n-k)!}\]
    \begin{itemize}
        \item [(a)] Evento A: Ottengo \[P(A) = \frac{\binom{25}{2}}{1225} = \frac{12}{49}\] dove 25 sono le palline dispari (50-25 = 25)
        \item [(b)] Evento B: Ottengo \[P(B) = \frac{10*40}{1225} = \frac{16}{49}\] dove 10 sono i numeri \textbf{divisibili} per 5 e 40 sono i numeri \textbf{non divisibili} per 5
        \item [(c)] Evento C: Ottengo \[P(C) = \{1,49\}, \{2, 48\}, ... , \{24, 26\} = 24 \xrightarrow{porta\ al\ risultato} \frac{24}{1225}\]
    \end{itemize} 
    \paragraph{Soluzione Alternativa} Si può risolvere tenendo conto dell' ordine, considerando \(\overline{\Omega}\) come l'insieme delle disposizioni di 2 palline scelte dalle 50 dall'urna. Si ha: \[|\overline{\Omega}| = 50*49\]
    \begin{itemize}
        \item [(a)] Evento A: Ottengo \[P(A) = \frac{25*24}{50*49} = \frac{12}{49}\]
        \item [(b)] Evento B: Questo evento diventa l'\textbf{unione disgiunta} dei due eventi \begin{itemize}
            \item \(B_1\) = "il primo numero è divisibile per 5 e il secondo no"
            \item \(B_2\) = "il primo numero non è divisibile per 5 e il secondo si"
        \end{itemize}
        Ottengo \[P(B) = P(B_1 \cup B_2) = P(B_1) + P(B_2) = \frac{10*40}{50*49} + \frac{40*10}{50*49} = \frac{16}{49}\] 
        \item [(c)] Evento C: Le coppie ordinate che sommano a 50 sono (1,49), (49,1), ... (24,26), (26,24) \[P(C) = \frac{48}{50*49} = \frac{24}{1225}\]
    \end{itemize}
    \item Lanciamo 3 dadi equilibrati. Calcolare la probabilità di ottenere:
    \begin{itemize}
        \item [(a)] Tre numeri dispari
        \item [(b)] Due numeri pari e uno dispari
        \item [(c)] Tre numeri la cui somma è 5
        \item [(d)] Almeno due 1.
    \end{itemize}
    \paragraph{Soluzione:} Considero \[\Omega = \{(i, j, k): i, j, k \in \{1, ..., 6\}\}\] Questo spazio campionario \textbf{tiene conto dell'ordine}. Si ha \(|\Omega| = 6^3 = 216 esiti possibili\).
    \begin{itemize}
        \item [(a)] Evento A: si ha \[P(A) = \frac{3^3}{216} = \frac{1}{8}\]
        \item [(b)] Evento B: si ha \[P(B) = \frac{3^4}{216} = \frac{3}{8}\] Si ha \(3^4\) perchè si hanno 3 posizioni diverse per i dadi che vanno incluse nel calcolo della probabilità totale, oltre alle scelte da fare sui dadi.
        \item [(c)] Evento C: si ha\[P(C) = \frac{2*3}{216} = \frac{1}{36}\] Questo perchè si hanno solo 2 terne che rispondono alla richiesta (le terne "113" e "221") ed hanno 3 ordinamenti diversi.
        \item [(d)] Evento D: questo evento è l'unione disgiunta dei due seguenti eventi: \begin{itemize}
            \item \(D_1\) = "Ottengo esattamente due 1" 
            \item \(D_2\) = "Ottengo tre 1"
        \end{itemize} 
        si ha quindi \[P(D) = (D_1 \cup D_2) = \frac{3*5*1}{216} + \frac{1}{216} = \frac{2}{27}\] questo perchè: in \(D_1\) vi è un caso in cui non deve uscire "1" \((\frac{3*5}{6})\) mentre gli altri due casi sono \(\frac{1}{6}*\frac{1}{6}\);\\
        in \(D_2\) invece ho solo un caso in cui ottengo esattamente tre "1", quindi ho \(\frac{1}{6}*\frac{1}{6}*\frac{1}{6} = \frac{1}{216}\). 
    \end{itemize}
\end{enumerate}
\section{Birthday Problem}
E' anche detto \textit{paradosso dei compleanni}, in informatica esiste un attacco di tipo brute force chiamato "Birthday Attack".\\
Questo problema consiste nel calcolare la probabilità dell'evento\\ \(E_n\) = "in una classe di n bambini almeno 2 hanno lo stesso compleanno" con \(n \leq 365\).\\
L'anno è composto da 365 giorni (no anno bisestile), che identifico con i numeri da 1 (1 Gennaio) a 365 (31 Dicembre). Lo spazio campionario sarà: \[\Omega_n = \{1, ..., 365\}^n\] e la misura di probabilità è \textbf{uniforme}. Dobbiamo calcolare \[P(E_n) = \frac{|E_n|}{|\Omega_n|} = \frac{|E_n|}{365^n}\] Il calcolo di \(|E_n|\) è complicato, quindi passo all'evento complementare, ossia\\ \(E_n^{c}\) = "in una classe di n bambini tutti i compleanni avvengono in giorni distinti". Si ha \[P(E_n) = 1 - P(E_n^{c}) = 1 - \frac{365*(365 - n + 1)}{365^n}\] Quello che ci interessa è calcolare \(n_*\) definito come il primo n per cui \(P(E_n) \geq \frac{1}{2}\), cioè \[n_* = min\{n\ |\ P(E_n) \geq \frac{1}{2}\}\] Ciò lo si vede con un grafico fatto con un calcolatore e si trova che \(n_* = 23\), quindi basta un numero piccolissimo di persone per avere una alta probabilità di persone con lo stesso compleanno all'interno del gruppo. \begin{figure}[hb]
    \centering
    \includegraphics[width = 9cm]{GraficoBirthday.png}
    \caption{Questo è il grafico che permette di vedere che quale è il minimo \(n_*\) da prendere in considerazione}
    \label{fig:BirthdayAttack}
\end{figure}
\chapter{Lezione 5}
\section{Probabilità Condizionata}
\paragraph{Esempio} Urna: 7 palline nere e 3 palline rosse. Estraggo 2 palline. Qual è la probabilità che la seconda pallina estratta sia rossa?\\
La risposta dipende da \textbf{quale pallina ho estratto alla prima estrazione}: la prima estrazione può avere 2 esiti diversi \begin{itemize}
    \item Pallina nera: resteranno \textbf{6 nere} e \textbf{3 rosse}, la probabilità che la seconda pallina sia rossa è \(\frac{1}{3}\)
    \item Pallina rossa: resteranno \textbf{7 nere} e \textbf{2 rosse}, la probabilità che la seconda pallina sia rossa è \(\frac{2}{9}\)
\end{itemize}
quindi la probabilità \textbf{cambia a seconda della scelta} che faccio.\\
Con questo concetto rispondiamo alle affermazioni del tipo: "la probabilità di E sapendo che si è verificato l'evento F".
\section{Definizione Probabilità Condizionata}
Siano \((\Omega, \textit{F}, P)\) uno spazio di probabilità e \(F \in \textit{F}\) un evento tale che \(P(F) > 0\).\\
Allora, per ogni altro evento \(E \in \textit{F}\) è ben definita la quantità \[P(E|F) = \frac{P(E \cap F)}{P(F)}\] Questa è la probabilità condizionata di \textbf{E dato F}.
\paragraph{Osservazione:} Se P(E) e P(F) sono entrambe strettamente positive, allora \[P(E \cap F) = P(E|F)*P(F) = P(F|E)*P(E)\] Questa formula si chiama \textbf{formula di moltiplicazione}, è un trucco per calcolare l'intersezione.
\paragraph{Giustificazione dell'osservazione:} Mettiamoci su uno spazio di probabilità \textbf{uniforme} e supponiamo di avere un'urna fatta in questo modo: \begin{itemize}
    \item Ci sono 8 palline numerate da 1 a 8
    \item Le palline "3", "5" e "8" sono nere
    \item Le restanti palline sono rosse
\end{itemize}
Consideriamo gli eventi: \begin{itemize}
    \item E = "estraggo una pallina con numero pari"
    \item F = "estraggo una pallina rossa"
\end{itemize}
\(P(E) = \frac{1}{2}\). Come cambia la situazione se voglio calcolare \(P(E|F)\)? \[P(E|F) = \frac{P(E \cap F)}{P(F)} = \frac{|E \cap F|}{|\Omega|}*\frac{|\Omega|}{|F|} = \frac{|E \cap F|}{|F|} = \frac{3}{5}\] Questa ultima espressione risponde all'affermazione "casi favorevoli su casi possibili", ma in uno \textbf{spazio di probabilità ridotto}. Quindi si può dire che \textbf{condizionare la scelta riduce lo spazio degli esiti possibili}.
\paragraph{Osservazione} \(P(E|F)\) è \textbf{minore, maggiore o uguale} a \(P(E)\).
\subsection{Teorema}
Sia \(F \in \textit{F}\) un evento con \(P(F) > 0\), allora la mappa \[P(\cdot |F): \textit{F} \xrightarrow{} \mathbb{R}\] \[E \longmapsto P(E|F)\] è una misura di probabilità. Da ciò seguono due conseguenze importanti: \begin{enumerate}
    \item \(P(E^c|F) = 1- P(E|F)\)
    \item \(P(E \cup G|F = P(E|F) + P(G|F) - P(E \cap G|F)\), è la formula di \textbf{inclusione/esclusione}
\end{enumerate}
\paragraph{Attenzione:} Fissato \(E \in \textit{F}\), la mappa di \[P(E|\cdot ): \textit{F} \xrightarrow{} \mathbb{R}\] \[F \longmapsto P(E|F)\] \textbf{non è} una misura di probabilità e quindi \[P(E|F^c) \neq 1 - P(E|F)\]
\section{Formula delle probabilità totali}
\subsection{Teorema}
Siano (\(\Omega, \textit{F}, P\)) uno spazio di probabilità e \(F \in \textit{F}\) un evento tale che \(0 < P(F) < 1\), quindi un evento non impossibile e non certo, allora, per ogni \(E \in \textit{F}\), vale la formula delle probabilità totali: \[P(E) = P(E|F)\cdot P(F) + P(E|F^c)\cdot P(F^c)\] In generale, se ho \(\{F_k\}^n_{k = 1}\) che è una partizione di \(\Omega\) con \(P(F_k) > 0\) per ogni k, vale la formula delle probabilità totali: \[P(E) = \sum^n_{k=1} P(E|F_k)\cdot P(F_k)\]
\paragraph{Dimostrazione:} Dimostriamo il caso generale. Scomponiamo l'evento E sulla partizione \(\{F_k\}^{n}_{k=1}\) ed otteniamo: \[E = \bigcup^{n}_{k=1} (E \cap F_k)\] ovvero una unione disgiunta. Quindi calcoliamo: \[P(E) = P(\bigcup^{n}_{k=1} (E \cap F_k)) = \sum^n_{k=1} P(E \cap F_k)\] che ci porta a \[\sum^n_{k=1} P(E|F_k)\cdot P(F_k)\] c.v.d.
\subsubsection{Esempio}
Urna del primo esempio: 7 palline nere e 3 rosse. Estraggo 2 palline e devo determinare quale è la probabilità che la seconda pallina sia rossa.\\
Definisco gli eventi: \(R_i\) = "l'i-esima pallina estratta è rossa", con i = 1,2 ed ottengo: \[P(R_2) = P(R_2|R_1)\cdot P(R_1) + P(R_2|R_1^c)\cdot P(R_1^c) = \frac{2}{9}\cdot \frac{3}{10} + \frac{1}{3}\cdot \frac{7}{10} = \frac{3}{10}\] dove le frazioni indicano, rispettivamente: \begin{itemize}
    \item La probabilità della seconda pallina estratta dopo aver estratto una rossa (\(\frac{2}{9}\))
    \item La probabilità di estrarre una pallina rossa alla prima estrazione (\(\frac{3}{10}\))
    \item La probabilità di avere una pallina rossa dopo una pallina nera (\(\frac{1}{3}\))
    \item La probabilità di estrarre una pallina nera alla prima estrazione (\(\frac{7}{10}\))
\end{itemize}
\subsection{Esercizio: Tre Urne - Prima parte} \label{EsercizioTreUrne1}
Ho tre urne. \begin{itemize}
    \item Prima urna: 3 palline bianche e 2 nere
    \item Seconda urna: 3 palline bianche e 3 nere
    \item Terza urna: 4 palline bianche e 1 nera
\end{itemize}
Lancio un dado equilibrato: \begin{itemize}
    \item Se esce \textbf{6} estraggo una pallina dalla \textbf{terza urna}
    \item Se esce \textbf{4 o 5} estraggo dalla \textbf{seconda urna}
    \item Negli altri casi estraggo dalla \textbf{prima urna}
\end{itemize}
Quale è la probabilità di estrarre una pallina bianca?
\paragraph{Soluzione} Consideriamo gli eventi \(U_i\) = "peschiamo dall'urna i", con i = 1,2,3 e l'evento B = "peschiamo una pallina bianca". Abbiamo: \begin{itemize}
    \item \(P(U_1) = P(punteggio\ dado \in \{1,2,3\}) = \frac{1}{2}\)
    \item \(P(U_2) = P(punteggio\ dado \in \{4,5\}) = \frac{1}{3}\)
    \item \(P(U_3) = P(punteggio\ dado = 6) = \frac{1}{6}\)
\end{itemize}
Dal testo ci possiamo anche ricavare le probabilità condizionate: \begin{itemize}
    \item \(P(B|U_1) = \frac{3}{5}\)
    \item \(P(B|U_2) = \frac{1}{2}\)
    \item \(P(B|U_3) = \frac{4}{5}\)
\end{itemize}
Quindi, per la formula delle probabilità totali otteniamo: \[P(B) = P(B|U_1)\cdot P(U_1) + P(B|U_2)\cdot P(U_2) + P(B|U_3)\cdot P(U_3)\] che equivale a \[\frac{3}{5}\cdot \frac{1}{2} + \frac{1}{2}\cdot \frac{1}{3} + \frac{4}{5}\cdot \frac{1}{6} = \frac{18}{30} = \frac{3}{5}\]
\subsection{Formula di Bayes}
Questa formula serve per stimare le probabilità a posteriori, quindi a "rovesciare" il condizionamento. Questo avviene da
\[P(F_k|E) = \frac{P(F_k \cap E)}{P(E)}\cdot \frac{P(F_k)}{P(F_k)}\] Dati questi elementi posso capire che \[\frac{P(F_k \cap E)}{P(F_k)} = \frac{P(E|F_k)\cdot P(F_k)}{P(E)}\] ed è questa ultima parte (dopo l'uguale) la formula di Bayes.
\subsubsection{Esercizio: Tre Urne - Seconda Parte}
Tenendo le informazioni della sezione \ref{EsercizioTreUrne1}, sapendo che è stata estratta una pallina bianca, quale è la probabilità che sia stata estratta dalla terza urna?
\paragraph{Soluzione} Dobbiamo calcolare \(P(U_3|B)\). Da quanto visto prima sappiamo che \[P(U_3) = \frac{1}{6},\ P(B|U_3) = \frac{4}{5}\ e\ P(B) = \frac{3}{5}\] Per la formula di Bayes, si ha \[P(U_3|B) = \frac{P(B|U_3)\cdot P(U_3)}{P(B)} = \frac{\frac{4}{5}\cdot \frac{1}{6}}{\frac{3}{5}} = \frac{2}{9}\]
\subsection{Esercizi}
\begin{enumerate}
    \item Lungo un canale di trasmissione sono spediti dati in formato {0,1}. Il canale è disturbato e a volte capita che si trasmetta uno 0 e si riceva un 1 e vicenversa. Si stima che: \begin{itemize}
        \item Se si trasmette uno 0, viene ricevuto correttamente con probabilità 0.95
        \item Se si trasmette un 1, viene ricevuto correttamente con probabilità 0.75
        \item Si sa inoltre che il 70\% dei segnali trasmessi sono 0.
    \end{itemize}
    Viene spedito un segnale lungo il canale, calcolare la probabilità che:
    \begin{itemize}
        \item [a)] Venga ricevuto uno 0
        \item [b)] Si verifichi un errore di trasmissione
        \item [c)] Il segnale spedito fosse uno 0, sapendo che è stato ricevuto uno 0.
    \end{itemize}
    \paragraph{Soluzione} Definiamo gli eventi: \begin{itemize}
        \item \(T_i\) = "Trasmetto il segnale i", con \(i \in \{0, 1\}\)
        \item \(R_i\) = "Ricevo il segnale i", con \(i \in \{0, 1\}\)
    \end{itemize}
    Dal testo ricaviamo che: \[P(R_0|T_0) = 0.95,\ P(R_1|T_1) = 0.75\ e\ P(T_0) = 0.7\] 
    \begin{itemize}
        \item [a)] Dobbiamo calcolare la probabilità di \(P(R_0)\). Uso la formula delle probabilità totali: \[P(R_0) = P(R_0|T_0)\cdot P(T_0) + P(R_0|T_1)\cdot P(T_1)\] Si ha: \begin{itemize}
            \item \(P(R_0|T_1) = P(R_1^c|T_1) = 1 - P(R_1|T_1) = 1 - 0.75 = 0.25\)
            \item \(P(T_1) = P(T_0^c) = 1 - P(T_0) = 1 - 0.7 = 0.3\)
        \end{itemize}
        Sostituisco ed ottengo: \[(0.95\cdot 0.7) + (0.25 \cdot 0.3) = 0.74\]
        \item [b)] Sia E l'evento "Si verifica un errore di trasmissione". \[E = (T_0 \cap R_1) \cup (T_1 \cap R_0)\] Questa è una unione disgiunta, quindi ottengo: \[P(E) = P(T_0 \cap R_1) + P(T_1 \cap R_0) = P(R_1|T_0)\cdot P(T_0) + P(R_0|T_1)\cdot P(T_1)\]
        Si ha : \begin{itemize}
            \item \(P(R_1|T_0 = P(R_0^c|T_0) = 1 - P(R_0|T_0) = 1 - 0.95 = 0.05\)
            \item \(P(R_0|T_1 = P(R_1^c|T_1) = 1 - P(R_1|T_1) = 1 - 0.75 = 0.25\)
        \end{itemize}
        Infine ottengo che \[P(E) = (0.05)\cdot (0.7) + (0.25)\cdot (0.3) = 0.11\]
        \item [c)] Dobbiamo calcolare \(P(T_0|R_0)\). E' una probabilità \textbf{a posteriori}, quindi si usa la formula di Bayes e otteniamo: \[P(T_0|R_0) = \frac{P(R_0|T_0)\cdot P(T_0)}{P(R_0)}\] \[= \frac{(0.95) \cdot (0.7)}{0.74} = 0.9\] 
    \end{itemize}
    \item Siano A,B eventi tali che P(A) = \(\frac{1}{4}\) e \(P(A|B) = P(B|A) = \frac{1}{2}\). Si dica, giustificando, se è vero o falso che: \begin{itemize}
        \item [a)] A e B sono incompatibili
        \item [b)] \(P(A \cap B) = P(A)\cdot P(B)\)
        \item [c)] \(P(A \cap B) > P(A) \cdot P(B)\)
    \end{itemize}
    \paragraph{Soluzione}\begin{itemize}
        \item [a)] \(P(A \cap B) = P(B|A) \cdot P(A) = \frac{1}{8}\). Se A,B fossero incompatibili, si avrebbe \(A \cap B = \varnothing\) e quindi \(P(A \cap B) = 0\). \textbf{FALSO}.
        \item [b)] \(P(A \cap B) = P(A)\cdot P(B)\). Si ha che: \[P(A \cap B) = P(A|B)\cdot P(B)\]. Quindi posso semplificare per P(B) ed otterrei che \[P(A|B) = P(A) \xrightarrow{} \frac{1}{2} = \frac{1}{4}\] il che è \textbf{ASSURDO}. Quindi b) è \textbf{FALSO}.
        \item [c)] Dato \(P(A \cap B) > P(A) \cdot P(B)\), abbiamo, dopo la moltiplicazione per P(B) \[P(A|B) \cdot P(B) > P(A) \cdot P(B) = P(A \cap B) > P(A) \cdot P(B)\] e poichè se P(B) non fosse strettamente positivo, \(P(A|B)\) non esisterebbe nemmeno, concludiamo che c) è \textbf{VERO}.
    \end{itemize}
\end{enumerate}
\chapter{Lezione 6}
\section{Eventi Indipendenti}
\subsection{Indipendenza di due eventi}
Sia \((\Omega, \textit{F}, P)\) spazio di probabilità. Gli eventi \(E, F \in \textit{F}\) si dicono indipendenti se \[P(E \cap F) = P(E) \cdot P(F)\] e si scrive \[E \indep F\ o\ \{E, F\}\ indipendente\]
\paragraph{Attenzione} Indipendenza è \textbf{diverso da} incompatibilità. \begin{itemize}
    \item Indipendenza è una nozione \textbf{probabilistica}, dipende da E,F e dalla misura P.
    \item Incompatibilità è una nozione \textbf{insiemistica}.
\end{itemize}
In particolare, se E,F \(\in \textit{F}\) di probabilità strettamente positiva sono incompatibili, allora \textbf{non possono essere indipendenti}. Infatti, essendo incompatibili, significa che se uno si verifica l'altro \textbf{non può certamente verificarsi}, quindi c'è una \textbf{forte dipendenza} tra i due eventi.
\paragraph{Esempio (banale)}
Qualunque sia \(E \in \textit{F}\), allora \(E \indep \varnothing\ e\ E \indep \Omega\). Infatti, si ha \begin{itemize}
    \item \(P(E \cap \varnothing) = P(\varnothing) = 0 = P(\varnothing) \cdot P(E)\)
    \item \(P(E \cap \Omega) = P(E) = P(E) \cdot P(\Omega)\) dove \(P(\Omega) = 1\)
\end{itemize}
\paragraph{Esempi}
\begin{enumerate}
    \item Lanciamo una moneta e un dado. Lo spazio campionario naturale è \[\Omega = \{(a,i): a \in \{T,C\}, i \in \{1,...,6\}\}\] con la misura uniforme.\\
    Osserviamo che \(|\Omega| = 2\cdot 6 = 12\). Gli eventi: \begin{itemize}
        \item E = "Esce testa"
        \item F = "Esce 4"
    \end{itemize}
    sono indipendenti. Infatti, se calcoliamo \[P(E) = P(\{(T,i): i \in \{1,...,6\}\}) = \frac{1}{2}\] \[P(F) = P(\{(a, 4): a \in \{T,C\}\}) = \frac{1}{6}\] 
    \[P(E \cap F) = P(\{(T,4)\}) = \frac{1}{12}\] otteniamo che \(\frac{1}{12} = P(E \cap F) = P(E)\cdot P(F) = \frac{1}{2}\cdot \frac{1}{6}\)
    Questo ci permette di dire che gli eventi sono indipendenti.
    \item Lanciamo un dado due volte. Lo spazio campionario naturale sarà \[\Omega = \{(i,j): i,j \in \{1,...,6\}\}\] con la misura di probabilità \textbf{uniforme}, quindi tutti gli esiti sono equiprobabili.\\
    Osserviamo che \(|\Omega| = 6 \cdot 6 = 36\). Gli eventi:
    \begin{itemize}
        \item E = "La prima faccia è 4"
        \item F = "La somma dei due punteggi è 9" 
    \end{itemize}
    \textbf{non sono indipendenti}.
    \[P(E) = P(\{(4,j): j \in \{1,...,6\}\}) = \frac{1}{6}\]
    \[P(F) = P(\{(3,6), (4,5), (5,4), (6,3)\}) = \frac{1}{9}\]
    \[P(E \cap F) = P(\{(4,5)\}) = \frac{1}{36}\] poichè è solo una la coppia che rispetta la condizione.\\
    Otteniamo \(\frac{1}{36} = P(E \cap F) \neq P(E)\cdot P(F) = \frac{1}{6} \cdot \frac{1}{9} = \frac{1}{54}\), quindi gli eventi \textbf{non sono indipendenti}.\\
    Se invece prendessimo l'evento G = "la somma dei punteggi è 7 " e sostituissimo F con G, gli eventi E e G sarebbero indipendenti.
\end{enumerate}
\section{Conseguenze elementari dell'indipendenza}
\paragraph{Proposizione 1}Siano E,F \(\in \textit{F}\) con \(P(E) > 0\ e\ P(F) > 0\). Allora le seguenti affermazioni:
    \begin{itemize}
        \item [(i)] \(E \indep F\)
        \item [(ii)] \(P(E|F) = P(E)\)
        \item [(iii)] \(P(F|E) = P(F)\)
    \end{itemize}
    sono \textbf{equivalenti}.
\subsection{Dimostrazione} \begin{itemize}
    \item [(i) \(\xrightarrow{} (ii)\)] \(E \indep F \xrightarrow{implica} P(E \cap F) = P(E)\cdot P(F)\) \[P(E \cap F) = P(E|F) \cdot P(F)\] quindi posso semplificare i \(P(F)\) ed ottengo ciò che volevo dimostrare, ossia \[P(E|F) = P(E)\]
    \item [(ii) \(\xrightarrow{} (iii)\)] \(P(E|F) = P(E) \xrightarrow{(Bayes)} \frac{P(F|E)\cdot P(E)}{P(F)} = P(E)\) semplifico per P(E) e moltiplico per P(F), ottenendo \[P(F|E) = P(F)\]
    \item [(iii) \(\xrightarrow{} (i)\)] \(P(F|E) = P(F) \xrightarrow{implica} \frac{P(E \cap F)}{P(E)} = P(F)\) moltiplico per P(E) ed ottengo \[P(E \cap F) = P(F) \cdot P(E) \xrightarrow{ci\ porta\ a} E \indep F\] c.v.d
\end{itemize}
\subsection{Proposizione 2} Abbiamo \textbf{equivalenza} tra le seguenti istanze:
\begin{itemize}
    \item [(i)] \(E \indep F\)
    \item [(ii)] \(E^c \indep F\)
    \item [(iii)] \(E^c \indep F^c\)
    \item [(iv)] \(E \indep F^c\)
\end{itemize}
\paragraph{Dimostrazione} Abbiamo anche qui una catena di implicazioni: \begin{itemize}
    \item [(i) \(\xrightarrow{} (ii)\)] Dobbiamo mostrare che \(P(E^c \cap F) = P(E^c) \cdot P(F)\). \\
    Si ha \(E^c \cap F = F \backslash (E \cap F)\) ed inoltre si ha che \(E \cap F \subseteq F\).\\
    Quindi \(P(E^c \cap F) = P(F) - P(E \cap F)\). Usiamo l'ipotesi (i), quindi otteniamo: \[P(E^c \cap F) = P(F) - P(E) \cdot P(F) = P(F)\cdot (1-P(E))\] ed \((1-P(E)) = P(E^c)\), quindi \[P(E^c \cap F) = P(E^c)\cdot P(F)\]
    Per concludere si deve dimostrare che \((ii) \xrightarrow{}(iii)\xrightarrow{}(iv)\xrightarrow{}(i)\).
\end{itemize}
\section{Indipendenza di Tre Eventi}
Gli eventi \(E_1, E_2, E_3\) sono indipendenti se le seguenti due condizioni sono \textbf{entrambe} soddisfatte: \begin{enumerate}
    \item \(E_1 \indep E_2, E_1 \indep E_3, E_2 \indep E_3\)
    \item \(P(E_1 \cap E_2 \cap E_3) = P(E_1)\cdot P(E_2) \cdot P(E_3)\)
\end{enumerate}
\paragraph{Osservazione} Le due condizioni \textbf{non sono rindondanti}, sono infatti logicamente \textbf{indipendenti} e necessarie.
\subsection{Esempi}
\begin{enumerate}
    \item Lancio due volte una moneta. Considero gli eventi: \begin{itemize}
        \item A = "al primo lancio ottengo testa"
        \item B = "al secondo lancio ottengo testa"
        \item C = "nei due lanci ottengo due facce uguali"
    \end{itemize}
    Mostriamo che gli eventi sono a 2 a 2 indipendenti, ovvero che \textbf{vale la prima condizione} ma la seconda non è soddisfatta:
    \paragraph{Soluzione} Lo spazio campionario \(\Omega = \{(l_1, l_2): l_1,l_2 \in \{T,C\}\}\) con \(|\Omega| = 2\cdot 2 = 4\)
    \begin{itemize}
        \item Evento A: \(P(A) = P(\{(T,l_2): l_2 \in \{T,C\}\}) = \frac{1}{2}\)
        \item Evento B: \(P(B) = P(\{(l_1, T): l_1 \in \{T,C\}\}) = \frac{1}{2}\)
        \item Evento C: \(P(C) = P(\{(T, T), (C,C)\}) = \frac{1}{2}\)
    \end{itemize}
    Inoltre si ha: \[P(A \cap B) = P(A \cap C) = P(B \cap C) = P(\{(T,T)\} = \frac{1}{4}\]
    Si vede facilmente che le coppie \{A,B\}, \{A,C\} e \{B,C\} sono indipendenti.\\
    Per esempio prendiamo: \[\frac{1}{4} = P(A \cap B) = P(A) \cdot P(B) = \frac{1}{2}\cdot \frac{1}{2}\] le altre sono analoghe.
    Mostriamo ora che \[P(A \cap B \cap C) \neq P(A)\cdot P(B)\cdot P(C)\]
    \begin{itemize}
        \item \(P(A)\cdot P(B)\cdot P(C) = \frac{1}{8}\)
        \item \(P(A \cap B \cap C) = P(\{(T,T)\}) = \frac{1}{4}\)
    \end{itemize}
    \item Lancio due volte un dado. Considero gli eventi: \begin{itemize}
        \item A = "Il punteggio del primo lancio \(\in \{1,2,3\}\)"
        \item B = "Il punteggio del primo lancio \(\in \{3,4,5\}\)
        \item C = "La somma dei due punteggi è uguale a 9"
    \end{itemize}
    \paragraph{Soluzione} Mostriamo che: \begin{itemize}
        \item \(P(A\cap B \cap C) = P(A)\cdot P(B)\cdot P(C)\)
        \item Gli eventi \textbf{non sono indipendenti} a 2 a 2.
    \end{itemize}
    Lo spazio campionario è \[\Omega = \{(d_1, d_2): d_1,d_2 \in \{1,...,6\}\} \xrightarrow{} |\Omega| = 6\cdot 6 = 36\]
    Calcoliamo: \begin{itemize}
        \item \(P(A) = P(\{(d_1,d_2): d_1 \in \{1,2,3\}, d_2 \in \{1,...,6\}\}) = \frac{3\cdot 6}{36} = \frac{1}{2}\)
        \item \(P(A) = P(\{(d_1,d_2): d_1 \in \{3,4,5\}, d_2 \in \{1,...,6\}\}) = \frac{3\cdot 6}{36} = \frac{1}{2}\)
        \item \(P(C) = P(\{(3,6), (4,5), (5,4), (6,3)\}) = \frac{1}{9}\)
    \end{itemize}
    Inoltre: \[P(A\cap B\cap C) = P(\{(3,6)\} = \frac{1}{36}\]
    Quindi \[P(A\cap B\cap C) = \frac{1}{36} = P(A)\cdot P(B)\cdot P(C) = \frac{1}{2}\cdot \frac{1}{2}\cdot \frac{1}{9} = \frac{1}{36}\] 
    Consideriamo ora \{A,C\} e mostriamo che non sono indipendenti. Sappiamo già che: \[P(A)\cdot P(C) = \frac{1}{2}\cdot \frac{1}{9} = \frac{1}{18}\]
    Calcoliamo \[P(A\cap C) = P(\{3,6\}) = \frac{1}{36}\]
    Si può facilmente vedere che \[\frac{1}{18} \neq \frac{1}{36}\]
    quindi gli eventi \textbf{non sono a due a due indipendenti}. Si può procedere in modo analogo per le altre coppie, ma il risultato è lo stesso: basta che solo una coppia non soddisfi la condizione e anche le altre non la soddisferanno.
\end{enumerate}
\subsection{In generale}
La famiglia di \(\{E_1,...,E_n\}\) di eventi è indipendente se \textbf{per ogni r} (\(2 \leq r \leq n\)) e per \textbf{ogni possibile scelta di r eventi distinti} degli n eventi della famiglia, la probabilità dell'intersezione degli r eventi scelti è \textbf{pari al prodotto delle loro probabilità}.\\
Devo quindi dimostrare la \textbf{proprietà di fattorizzazione} per ogni coppia, terna, quaterna, ecc. di eventi. Devo considerare \textbf{tutte le possibili sottofamiglie}.
\paragraph{Osservazione} Se \(\{E_1,...,E_n\}\) è una famiglia di eventi indipendenti, quando sostituisco qualche \(E_i\) (non importa quanti e quali) con \(E_i^c\) ottengo ancora una famiglia di eventi \textbf{indipendenti}.\\
La famiglia \textbf{numerabile} di eventi \(\{E_1,E_2,...\}\) è indipendente se ogni sottofamiglia finita lo è.
\end{document}
